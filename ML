import numpy as np
import pandas as pd
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, roc_curve, auc
from imblearn.combine import SMOTETomek
from xgboost import XGBClassifier
import matplotlib.pyplot as plt

# Create StratifiedKFold object
skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

# Initialize evaluation indicators
accuracies = []
f1_scores = []
precisions = []
recalls = []
mean_fpr = np.linspace(0, 1, 100)
tprs = []
aucs = []

# Hierarchical cross validation
for train_index, test_index in skf.split(X, y):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    
    smote_tomek = SMOTETomek(random_state=42)
    X_train_resampled, y_train_resampled = smote_tomek.fit_resample(X_train, y_train)

    # Training XGBoost model
    model = XGBClassifier(n_estimators=95,
                          max_depth=9, learning_rate=0.1, colsample_bytree=0.8,
                          subsample=0.5, alpha=0.008, colsample_bylevel=1, min_child_weight=0.3)
    model.fit(X_train_resampled, y_train_resampled)

    # predicting
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]

    
    accuracies.append(accuracy_score(y_test, y_pred))
    f1_scores.append(f1_score(y_test, y_pred, average='macro'))
    precisions.append(precision_score(y_test, y_pred, average='macro'))
    recalls.append(recall_score(y_test, y_pred, average='macro'))

    
    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
    roc_auc = auc(fpr, tpr)
    aucs.append(roc_auc)
    tprs.append(np.interp(mean_fpr, fpr, tpr))
    tprs[-1][0] = 0.0
    plt.plot(fpr, tpr, lw=2, alpha=0.3, label=f'ROC fold {len(tprs)} (AUC = {roc_auc:.2f})')

print(f"Average accuracy: {np.mean(accuracies)}")
print(f"Average F1 score: {np.mean(f1_scores)}")
print(f"Average precision: {np.mean(precisions)}")
print(f"Average recall: {np.mean(recalls)}")
print(f"Average AUC: {np.mean(aucs)}")

#Draw the average ROC curve
plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', alpha=.8)

mean_tpr = np.mean(tprs, axis=0)
mean_tpr[-1] = 1.0
mean_auc = auc(mean_fpr, mean_tpr)
std_auc = np.std(aucs)
plt.plot(mean_fpr, mean_tpr, color='greenyellow',  # Change color to green
         label=f'Mean ROC (AUC = {mean_auc:.2f})',
         lw=2, alpha=.8)  # Decrease linewidth to 2

plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', alpha=.8)

plt.xlim([-0.05, 1.05])
plt.ylim([-0.05, 1.05])
plt.xlabel('FPR',fontsize=14)
plt.ylabel('TPR',fontsize=14)
plt.tick_params(axis='both', which='major', labelsize=12)
# Update legend properties
legend = plt.legend(loc='lower right', prop={'size':10}, frameon=False) 
plt.show()

print(f"\n10-Fold Cross Validation AUC Values: {aucs}")
print(f"Mean AUC: {mean_auc:.2f} Â± {std_auc:.2f}")
